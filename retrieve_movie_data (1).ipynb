{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries and Set Up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "NYT_API_KEY=\"EVdZqo5d74afd0baQEdvF7I4d1ASD9Rc\"\n",
    "TMDB_API_KEY=\"2103f54b8c28fe44793106711dd04b68\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables from the .env in the local environment\n",
    "###Day 2 Activity 2###\n",
    "\n",
    "#load_dotenv() does not work for me, so I have to manually set the environment variables\n",
    "\n",
    "NYT_API_KEY=\"EVdZqo5d74afd0baQEdvF7I4d1ASD9Rc\"\n",
    "TMDB_API_KEY=\"2103f54b8c28fe44793106711dd04b68\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the New York Times API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################Day 2 Activity 6 for the next 2 cells####\n",
    "\n",
    "# Set the base URL\n",
    "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
    "\n",
    "###################################################################requires you to read the NYT documentation####\n",
    "# Filter for movie reviews with \"love\" in the headline\n",
    "# section_name should be \"Movies\"\n",
    "# type_of_material should be \"Review\"\n",
    "##############################################search for \"Filter Query Examples\" in the Article Search API docs.\n",
    "filter_query = 'section_name:\"Movies\" AND type_of_material:\"Review\" AND headline:\"love\"'\n",
    "\n",
    "# Use a sort filter, sort by newest\n",
    "sort = \"newest\"\n",
    "\n",
    "# Select the following fields to return:\n",
    "# headline, web_url, snippet, source, keywords, pub_date, byline, word_count\n",
    "###############################################look for \"Limit Fields in Response in the Article Search API Docs\"\n",
    "field_list = \"headline,web_url,snippet,source,keywords,pub_date,byline,word_count\"\n",
    "\n",
    "# Search for reviews published between a begin and end date\n",
    "begin_date = \"20130101\"\n",
    "end_date = \"20230531\"\n",
    "\n",
    "# Build URL\n",
    "query_url= f'{url}api-key={NYT_API_KEY}&q={filter_query}&begin_date={begin_date}&end_date={end_date}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 retrieved\n",
      "Page 1 retrieved\n",
      "Page 2 retrieved\n",
      "Page 3 retrieved\n",
      "Page 4 retrieved\n",
      "Page 5 retrieved\n",
      "Page 6 retrieved\n",
      "Page 7 retrieved\n",
      "Page 8 retrieved\n",
      "Page 9 retrieved\n",
      "Page 10 retrieved\n",
      "Page 11 retrieved\n",
      "Page 12 retrieved\n",
      "Page 13 retrieved\n",
      "Page 14 retrieved\n",
      "Page 15 retrieved\n",
      "Page 16 retrieved\n",
      "Page 17 retrieved\n",
      "Page 18 retrieved\n",
      "Page 19 retrieved\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the reviews\n",
    "reviews_list = []\n",
    "\n",
    "# loop through pages 0-19\n",
    "for page in range(0,20):\n",
    "    query_url= f'{url}api-key={NYT_API_KEY}&q={filter_query}&begin_date={begin_date}&end_date={end_date}'\n",
    "    # create query with a page number\n",
    "    # API results show 10 articles at a time\n",
    "    query_url = f'{query_url}&page={page}'\n",
    "    \n",
    "    # Make a \"GET\" request and retrieve the JSON\n",
    "    reviews= requests.get(query_url).json()\n",
    "    \n",
    "    # Add a twelve second interval between queries to stay within API query limits\n",
    "    time.sleep(12)\n",
    "    \n",
    "    # Try and save the reviews to the reviews_list\n",
    "    try:\n",
    "        # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
    "        for review in reviews[\"response\"][\"docs\"]:\n",
    "            reviews_list.append(review)\n",
    "        # Print the page that was just retrieved\n",
    "        print(f\"Page {page} retrieved\")\n",
    "\n",
    "        # Print the page number that had no results then break from the loop\n",
    "    except:\n",
    "        print(f\"no reviews on page {page}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_df = pd.DataFrame(reviews_list)\n",
    "rl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"OK\",\n",
      "  \"copyright\": \"Copyright (c) 2023 The New York Times Company. All Rights Reserved.\",\n",
      "  \"response\": {\n",
      "    \"docs\": [],\n",
      "    \"meta\": {\n",
      "      \"hits\": 0,\n",
      "      \"offset\": 190,\n",
      "      \"time\": 25\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Page 19 retrieved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''#It looks like the issue might be with how the data is nested in the JSON response from the API. Specifically, the reviews_list is likely not being populated correctly due to the structure of the JSON response.\n",
    "#Here are a few suggestions to address the issue:\n",
    "#Review JSON Structure:\n",
    "#Examine the structure of the JSON response to understand how the review information is nested. You may need to navigate through different levels in the JSON to access the relevant data.\n",
    "Check Nested Structure:\n",
    "Verify that the key paths used to access data in the loop match the actual structure of the JSON response. For example, if the reviews are nested inside [\"response\"][\"docs\"], make sure this is the case in the actual JSON.\n",
    "Print Debug Information:\n",
    "Add print statements within your loop to print out the structure of the JSON at different levels. This can help you identify the correct keys to access the data.\n",
    "Here's an example of how you can print out some information for debugging:\n",
    "python\n",
    "Copy code'''\n",
    "# Print the structure of the JSON response\n",
    "print(json.dumps(reviews, indent=2))\n",
    "\n",
    "# ...\n",
    "\n",
    "# Try and save the reviews to the reviews_list\n",
    "try:\n",
    "    # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
    "    for review in reviews[\"response\"][\"docs\"]:\n",
    "        print(review)  # Print each review for debugging\n",
    "        reviews_list.append(review)\n",
    "    # Print the page that was just retrieved\n",
    "    print(f\"Page {page} retrieved\")\n",
    "\n",
    "    # Print the page number that had no results then break from the loop\n",
    "except:\n",
    "    print(f\"no reviews on page {page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query URL: https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=EVdZqo5d74afd0baQEdvF7I4d1ASD9Rc&q=section_name:\"Movies\" AND type_of_material:\"Review\" AND headline:\"love\"&begin_date=20130101&end_date=20230531&page=19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''It seems that the issue lies in the fact that the API response is empty, as indicated by the \"hits\": 0 in the meta information. This means that the API did not return any documents that match your query for the specified time range and other filters.\n",
    "\n",
    "Possible reasons for not getting any results:\n",
    "\n",
    "No matching reviews:\n",
    "It's possible that there are no movie reviews with \"love\" in the headline within the specified date range and other criteria.\n",
    "\n",
    "Narrow search criteria:\n",
    "Your search criteria might be too restrictive, leading to no matching results. You could try broadening the search by removing some filters or using a different query.\n",
    "\n",
    "To debug and identify the issue:\n",
    "\n",
    "Check the query parameters:\n",
    "Print the constructed query_url before making the API request. This will allow you to manually inspect the URL and verify that the query parameters are correctly set.\n",
    "python\n",
    "Copy code'''\n",
    "print(f\"Query URL: {query_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Modify search criteria:\n",
    "Try modifying the search criteria to be less restrictive. For example, you could remove the headline:\"love\" filter and see if you get any results. If you do, you can gradually add filters back to identify which one is causing the lack of results.\n",
    "python\n",
    "Copy code'''\n",
    "filter_query = 'section_name:\"Movies\" AND type_of_material:\"Review\"'\n",
    "'''By investigating these aspects, you should be able to identify the reason why the API is not returning any reviews for your current query.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 retrieved\n",
      "Page 1 retrieved\n",
      "Page 2 retrieved\n",
      "Page 3 retrieved\n",
      "Page 4 retrieved\n",
      "Page 5 retrieved\n",
      "Page 6 retrieved\n",
      "Page 7 retrieved\n",
      "Page 8 retrieved\n",
      "Page 9 retrieved\n",
      "Page 10 retrieved\n",
      "Page 11 retrieved\n",
      "Page 12 retrieved\n",
      "Page 13 retrieved\n",
      "Page 14 retrieved\n",
      "Page 15 retrieved\n",
      "Page 16 retrieved\n",
      "Page 17 retrieved\n",
      "Page 18 retrieved\n",
      "Page 19 retrieved\n"
     ]
    }
   ],
   "source": [
    "filter_query = 'section_name:\"Movies\" AND type_of_material:\"Review\"'\n",
    "query_url= f'{url}api-key={NYT_API_KEY}&q={filter_query}&begin_date={begin_date}&end_date={end_date}'\n",
    "# Create an empty list to store the reviews\n",
    "reviews_list = []\n",
    "\n",
    "# loop through pages 0-19\n",
    "for page in range(0,20):\n",
    "    query_url= f'{url}api-key={NYT_API_KEY}&q={filter_query}&begin_date={begin_date}&end_date={end_date}'\n",
    "    # create query with a page number\n",
    "    # API results show 10 articles at a time\n",
    "    query_url = f'{query_url}&page={page}'\n",
    "    \n",
    "    # Make a \"GET\" request and retrieve the JSON\n",
    "    reviews= requests.get(query_url).json()\n",
    "    \n",
    "    # Add a twelve second interval between queries to stay within API query limits\n",
    "    time.sleep(12)\n",
    "    \n",
    "    # Try and save the reviews to the reviews_list\n",
    "    try:\n",
    "        # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
    "        for review in reviews[\"response\"][\"docs\"]:\n",
    "            reviews_list.append(review)\n",
    "        # Print the page that was just retrieved\n",
    "        print(f\"Page {page} retrieved\")\n",
    "\n",
    "        # Print the page number that had no results then break from the loop\n",
    "    except:\n",
    "        print(f\"no reviews on page {page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_df = pd.DataFrame(reviews_list)\n",
    "rl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"OK\",\n",
      "  \"copyright\": \"Copyright (c) 2023 The New York Times Company. All Rights Reserved.\",\n",
      "  \"response\": {\n",
      "    \"docs\": [],\n",
      "    \"meta\": {\n",
      "      \"hits\": 0,\n",
      "      \"offset\": 190,\n",
      "      \"time\": 20\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Page 19 retrieved\n"
     ]
    }
   ],
   "source": [
    "# Print the structure of the JSON response\n",
    "print(json.dumps(reviews, indent=2))\n",
    "\n",
    "# ...\n",
    "\n",
    "# Try and save the reviews to the reviews_list\n",
    "try:\n",
    "    # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
    "    for review in reviews[\"response\"][\"docs\"]:\n",
    "        print(review)  # Print each review for debugging\n",
    "        reviews_list.append(review)\n",
    "    # Print the page that was just retrieved\n",
    "    print(f\"Page {page} retrieved\")\n",
    "\n",
    "    # Print the page number that had no results then break from the loop\n",
    "except:\n",
    "    print(f\"no reviews on page {page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 retrieved\n",
      "Page 1 retrieved\n",
      "Page 2 retrieved\n",
      "Page 3 retrieved\n",
      "Page 4 retrieved\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "NYT_API_KEY=\"EVdZqo5d74afd0baQEdvF7I4d1ASD9Rc\"\n",
    "TMDB_API_KEY=\"2103f54b8c28fe44793106711dd04b68\"\n",
    "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
    "begin_date = \"20120101\"\n",
    "end_date = \"20230531\"\n",
    "\n",
    "filter_query = 'section_name:\"Movies\" AND type_of_material:\"Review\"'\n",
    "query_url= f'{url}api-key={NYT_API_KEY}&q={filter_query}&begin_date={begin_date}&end_date={end_date}'\n",
    "# Create an empty list to store the reviews\n",
    "reviews_list = []\n",
    "\n",
    "# loop through pages 0-19\n",
    "for page in range(0, 5):\n",
    "    query_url= f'{url}api-key={NYT_API_KEY}&q={filter_query}&begin_date={begin_date}&end_date={end_date}'\n",
    "    # create query with a page number\n",
    "    # API results show 10 articles at a time\n",
    "    query_url = f'{query_url}&page={page}'\n",
    "    \n",
    "    # Make a \"GET\" request and retrieve the JSON\n",
    "    reviews= requests.get(query_url).json()\n",
    "    \n",
    "    # Add a twelve second interval between queries to stay within API query limits\n",
    "    time.sleep(12)\n",
    "    \n",
    "    # Try and save the reviews to the reviews_list\n",
    "    try:\n",
    "        # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
    "        for review in reviews[\"response\"][\"docs\"]:\n",
    "            reviews_list.append(review)\n",
    "        # Print the page that was just retrieved\n",
    "        print(f\"Page {page} retrieved\")\n",
    "\n",
    "        # Print the page number that had no results then break from the loop\n",
    "    except:\n",
    "        print(f\"no reviews on page {page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_df = pd.DataFrame(reviews_list)\n",
    "rl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"OK\",\n",
      "  \"copyright\": \"Copyright (c) 2023 The New York Times Company. All Rights Reserved.\",\n",
      "  \"response\": {\n",
      "    \"docs\": [],\n",
      "    \"meta\": {\n",
      "      \"hits\": 0,\n",
      "      \"offset\": 40,\n",
      "      \"time\": 23\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Page 4 retrieved\n"
     ]
    }
   ],
   "source": [
    "# Print the structure of the JSON response\n",
    "print(json.dumps(reviews, indent=2))\n",
    "\n",
    "# ...\n",
    "\n",
    "# Try and save the reviews to the reviews_list\n",
    "try:\n",
    "    # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
    "    for review in reviews[\"response\"][\"docs\"]:\n",
    "        print(review)  # Print each review for debugging\n",
    "        reviews_list.append(review)\n",
    "    # Print the page that was just retrieved\n",
    "    print(f\"Page {page} retrieved\")\n",
    "\n",
    "    # Print the page number that had no results then break from the loop\n",
    "except:\n",
    "    print(f\"no reviews on page {page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 retrieved\n",
      "Page 1 retrieved\n",
      "Page 2 retrieved\n",
      "Page 3 retrieved\n",
      "Page 4 retrieved\n"
     ]
    }
   ],
   "source": [
    "filter_query = 'section_name:\"Movies\"'\n",
    "query_url = f'{url}api-key={NYT_API_KEY}&q={filter_query}&begin_date={begin_date}&end_date={end_date}'\n",
    "# Dependencies\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "NYT_API_KEY=\"EVdZqo5d74afd0baQEdvF7I4d1ASD9Rc\"\n",
    "TMDB_API_KEY=\"2103f54b8c28fe44793106711dd04b68\"\n",
    "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
    "begin_date = \"20120101\"\n",
    "end_date = \"20230531\"\n",
    "\n",
    "filter_query = 'section_name:\"Movies\"'\n",
    "query_url = f'{url}api-key={NYT_API_KEY}&q={filter_query}&begin_date={begin_date}&end_date={end_date}'\n",
    "# Create an empty list to store the reviews\n",
    "reviews_list = []\n",
    "\n",
    "# loop through pages 0-19\n",
    "for page in range(0, 5):\n",
    "    query_url= f'{url}api-key={NYT_API_KEY}&q={filter_query}&begin_date={begin_date}&end_date={end_date}'\n",
    "    # create query with a page number\n",
    "    # API results show 10 articles at a time\n",
    "    query_url = f'{query_url}&page={page}'\n",
    "    \n",
    "    # Make a \"GET\" request and retrieve the JSON\n",
    "    reviews= requests.get(query_url).json()\n",
    "    \n",
    "    # Add a twelve second interval between queries to stay within API query limits\n",
    "    time.sleep(12)\n",
    "    \n",
    "    # Try and save the reviews to the reviews_list\n",
    "    try:\n",
    "        # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
    "        for review in reviews[\"response\"][\"docs\"]:\n",
    "            reviews_list.append(review)\n",
    "        # Print the page that was just retrieved\n",
    "        print(f\"Page {page} retrieved\")\n",
    "\n",
    "        # Print the page number that had no results then break from the loop\n",
    "    except:\n",
    "        print(f\"no reviews on page {page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_df = pd.DataFrame(reviews_list)\n",
    "rl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"OK\",\n",
      "  \"copyright\": \"Copyright (c) 2023 The New York Times Company. All Rights Reserved.\",\n",
      "  \"response\": {\n",
      "    \"docs\": [],\n",
      "    \"meta\": {\n",
      "      \"hits\": 0,\n",
      "      \"offset\": 40,\n",
      "      \"time\": 13\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Page 4 retrieved\n"
     ]
    }
   ],
   "source": [
    "# Print the structure of the JSON response\n",
    "print(json.dumps(reviews, indent=2))\n",
    "\n",
    "# ...\n",
    "\n",
    "# Try and save the reviews to the reviews_list\n",
    "try:\n",
    "    # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
    "    for review in reviews[\"response\"][\"docs\"]:\n",
    "        print(review)  # Print each review for debugging\n",
    "        reviews_list.append(review)\n",
    "    # Print the page that was just retrieved\n",
    "    print(f\"Page {page} retrieved\")\n",
    "\n",
    "    # Print the page number that had no results then break from the loop\n",
    "except:\n",
    "    print(f\"no reviews on page {page}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I give up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview = reviews_list[:5]\n",
    "reviews_json = json.dumps(preview, indent=4)\n",
    "reviews_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert reviews_list to a Pandas DataFrame using json_normalize()\n",
    "############################################################Day 1 Activity 8####\n",
    "rl_df = pd.json_normalize(reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 10+ retries, and 40+hours debugging my failed attempts, I finally got the data to load into a dataframe. From there the whole process went horribly. \n",
    "# convinced there is more than one way to skin a cat, I tried to use the data wrangler extension in VS Code to clean the data.\n",
    "# In the following notebooks all code was completed using the Data Wrangler extension in VS Code. \n",
    "# Info about Data Wrangler can be found here: https://marketplace.visualstudio.com/items?itemName=IanKemp.data-wrangler\n",
    "# after the challenge is completed, my code be posted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the title from the \"headline.main\" column and save it to a new column \"title\"\n",
    "rl_df['title'] = rl_df['headline.main'].apply(lambda x: x['main'].split('\\u2018', 1)[-1].split('\\u2019 Review', 1)[0] if 'main' in x else None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'name' and 'value' from items in \"keywords\" column\n",
    "def extract_keywords(keyword_list):\n",
    "    extracted_keywords = \"\"\n",
    "    for item in keyword_list:\n",
    "        # Extract 'name' and 'value'\n",
    "        keyword = f\"{item['name']}: {item['value']};\" \n",
    "        # Append the keyword item to the extracted_keywords list\n",
    "        extracted_keywords += keyword\n",
    "    return extracted_keywords\n",
    "\n",
    "# Fix the \"keywords\" column by converting cells from a list to a string\n",
    "##################you just applying the provided function to the keywords column\n",
    "\n",
    "rl_df['keywords'] = rl_df['keywords'].apply(extract_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list from the \"title\" column using to_list()\n",
    "# These titles will be used in the query for The Movie Database\n",
    "title_list= rl_df['title'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access The Movie Database API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare The Movie Database query\n",
    "url = \"https://api.themoviedb.org/3/search/movie?query=\"\n",
    "TMDB_API_KEY=\"2103f54b8c28fe44793106711dd04b68\"\n",
    "\n",
    "tmdb_key_string = \"&api_key=\" + TMDB_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a request counter to sleep the requests after a multiple of 50 requests\n",
    "for page in range(0,50):\n",
    "    query_m_url = f\"{url}{page}{tmdb_key_string}\"\n",
    "time.sleep(12)\n",
    "# Create an empty list to store the results\n",
    "movies=[]\n",
    "# Loop through the titles\n",
    "for m_title in title_list:\n",
    "    query_m_url = f\"{url}{m_title}{tmdb_key_string}\"\n",
    "    # Check if we need to sleep before making a request\n",
    "    if sleep:\n",
    "        time.sleep(12)\n",
    "        \n",
    "    # Perform a \"GET\" request for The Movie Database\n",
    "    # Add 1 to the request counter\n",
    "    response = requests.get(query_m_url).json()\n",
    "    req_cnt += 1\n",
    "    \n",
    "    # Include a try clause to search for the full movie details.\n",
    "    # Use the except clause to print out a statement if a movie\n",
    "    # is not found.\n",
    "    try:\n",
    "        # Get movie id\n",
    "        movie_id = response[\"results\"][0][\"id\"]\n",
    "\n",
    "        # Make a request for a the full movie details\n",
    "        movie_url = (f\"https://api.themoviedb.org/3/movie/{movie_id}?api_key={TMDB_API_KEY}\")\n",
    "\n",
    "        # Execute \"GET\" request with url\n",
    "        movie_response = requests.get(movie_url).json()\n",
    "        \n",
    "        # Extract the genre names into a list\n",
    "        genres = []\n",
    "\n",
    "        # Extract the spoken_languages' English name into a list\n",
    "        spoken_languages = []\n",
    "\n",
    "        # Extract the production_countries' name into a list\n",
    "        production_countries = []\n",
    "\n",
    "        # Add the relevant data to a dictionary and\n",
    "        # append it to the movies list\n",
    "        movies.append({\n",
    "            \"title\": movie_response[\"title\"],\n",
    "            \"release_date\": movie_response[\"release_date\"],\n",
    "            \"budget\": movie_response[\"budget\"],\n",
    "            \"revenue\": movie_response[\"revenue\"],\n",
    "            \"genres\": genres,\n",
    "            \"spoken_languages\": spoken_languages,\n",
    "            \"production_countries\": production_countries\n",
    "        })\n",
    "        \n",
    "        # Print out the title that was found\n",
    "        print(f\"Movie found: {movie_response['title']}\")\n",
    "        \n",
    "    except:\n",
    "        print(f\"Movie not found for title: {m_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first 5 results in JSON format\n",
    "# Use json.dumps with argument indent=4 to format data\n",
    "m_title = movies[:5]\n",
    "m_title_json = json.dumps(m_title, indent=4)\n",
    "reviews_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "m_df = pd.json_normalize(m_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and Clean the Data for Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge the New York Times reviews and TMDB DataFrames on title\n",
    "merged_df = pd.merge(rl_df, m_df, on='title', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove list brackets and quotation marks on the columns containing lists\n",
    "# Create a list of the columns that need fixing\n",
    "columns_to_fix = ['column1_faulty', 'column2_faulty', 'column3_faulty']  #\n",
    "for column in columns_to_fix:\n",
    "    merged_df[column] = merged_df[column].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else x)\n",
    "# Create a list of characters to remove\n",
    "remove_c= ['[',']', '\"']\n",
    "\n",
    "# Loop through the list of columns to fix\n",
    "for columns in columns_to_fix:\n",
    "    # Convert the column to type 'str'\n",
    "    merged_df[columns] = merged_df[columns].astype(str)\n",
    "\n",
    "    # Loop through characters to remove\n",
    "    for character in remove_c:\n",
    "            merged_df[columns] = merged_df[columns].str.replace(character, '')\n",
    "\n",
    "# Display the fixed DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"byline.person\" column\n",
    "merged_df = merged_df.dropna(columns=['byline.person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate rows and reset index\n",
    "merged_df= merged-df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV without the index\n",
    "merged_df.to_csv(\"output/merged_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
